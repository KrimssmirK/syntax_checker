{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an experiment of how to use `rply.LexerGenerator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the necessary libraries\n",
    "\n",
    "- rply.LexerGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rply import LexerGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LexerGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexers are generated by adding rules with regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg.add('NUMBER', r'\\d+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg.ignore(r\"\\s+\")\n",
    "lg.ignore(r\"_\")\n",
    "lg.ignore(r\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = lg.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('NUMBER', '12')\n",
      "Token('NUMBER', '1')\n",
      "Token('NUMBER', '1')\n",
      "Token('NUMBER', '1')\n"
     ]
    }
   ],
   "source": [
    "INPUT = \"12_;1_;1_;1\"\n",
    "for token in l.lex(INPUT):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if there is no rule added, it will be an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('NUMBER', '1')\n"
     ]
    },
    {
     "ename": "LexingError",
     "evalue": "(None, SourcePosition(idx=2, lineno=1, colno=1))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLexingError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m INPUT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1_+++1_;1_;1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Hantverk-Syntax-Checker/.venv/lib/python3.11/site-packages/rply/lexer.py:62\u001b[0m, in \u001b[0;36mLexerStream.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Hantverk-Syntax-Checker/.venv/lib/python3.11/site-packages/rply/lexer.py:58\u001b[0m, in \u001b[0;36mLexerStream.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m token\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LexingError(\u001b[38;5;28;01mNone\u001b[39;00m, SourcePosition(\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_colno))\n",
      "\u001b[0;31mLexingError\u001b[0m: (None, SourcePosition(idx=2, lineno=1, colno=1))"
     ]
    }
   ],
   "source": [
    "INPUT = \"1_+++1_;1_;1\"\n",
    "for token in l.lex(INPUT):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a sample tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is how to construct a tokenizer for the following snippets\n",
    "\n",
    "```\n",
    "str txt = \"Sample String\";\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = 'str txt = \"Sample String\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lg = LexerGenerator()\n",
    "\n",
    "my_lg.add(\"dataType\", r\"str\")\n",
    "my_lg.add(\"string\", r'\".*\"')\n",
    "my_lg.add(\"variable\", r\"[a-zA-Z][a-zA-Z0-9_]*\")\n",
    "my_lg.add(\"assignment\", r\"=\")\n",
    "\n",
    "my_lg.ignore(r\"\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_l = my_lg.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('dataType', 'str')\n",
      "Token('variable', 'txt')\n",
      "Token('assignment', '=')\n",
      "Token('string', '\"Sample String\"')\n"
     ]
    }
   ],
   "source": [
    "for token in my_l.lex(INPUT):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Rules based on BNF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ebnf_1.png\" />\n",
    "<img src=\"img/ebnf_2.png\" />\n",
    "<img src=\"img/ebnf_3.png\" />\n",
    "<img src=\"img/ebnf_4.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "RULES = {\n",
    "    \"singleLineComment\": r\"^#.*;$\",\n",
    "    \"multiLineComment\": r\"^#-{2,}.*-{2,}#;$\",\n",
    "    \"string\": r\"\\\".*\\\"|'.*'\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLexerGenerator:\n",
    "    def __init__(self):\n",
    "        self.lg = LexerGenerator()\n",
    "        self.is_rule_added = False\n",
    "    def add_rules(self, rules):\n",
    "        for key, value in rules.items():\n",
    "            self.lg.add(key, value)\n",
    "        self.lg.ignore(r\"\\s+\")\n",
    "        self.is_rule_added = True\n",
    "    def build(self):\n",
    "        if self.is_rule_added:\n",
    "            return self.lg.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lexer_generator = MyLexerGenerator()\n",
    "my_lexer_generator.add_rules(RULES)\n",
    "my_lexer = my_lexer_generator.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CASES = {\n",
    "    \"singleLineComment#1\": \"# asdfsd;\",\n",
    "    \"singleLineComment#2\": \"# ;\",\n",
    "    \"singleLineComment#3\": \"#;\",\n",
    "    \"singleLineComment#4\": \"#-;\",\n",
    "    \"singleLineComment#5\": \"#--;\",\n",
    "    \"singleLineComment#6\": \"#-- asddsf;\",\n",
    "    \"singleLineComment#7\": \"#-- asddsf #;\",\n",
    "    \"singleLineComment#8\": \"#-- asddsf -#;\",\n",
    "    \"string#1\": \"\\\"\\\"\",\n",
    "    \"string#2\": \"\\\" \\\"\",\n",
    "    \"string#3\": \"\\\"sdfs\\\"\",\n",
    "    \"string#4\": \"\\\"sdfs \\\"\",\n",
    "    \"string#5\": \"\\\"sdfs fssf\\\"\",\n",
    "    \"string#6\": \"\\\"2342\\\"\",\n",
    "    \"string#7\": \"''\",\n",
    "    \"string#8\": \"' '\",\n",
    "    \"string#9\": \"'sdfs'\",\n",
    "    \"string#10\": \"'sdfs '\",\n",
    "    \"string#11\": \"'sdfs fssf'\",\n",
    "    \"string#12\": \"'2342'\",\n",
    "    \"multiLineComment#1\": \"#----#;\",\n",
    "    \"multiLineComment#2\": \"#-- --#;\",\n",
    "    \"multiLineComment#3\": \"#--a--#;\",\n",
    "    \"multiLineComment#4\": \"#--afasddsf--#;\",\n",
    "    \"multiLineComment#5\": \"#--afas13 dfsa ddsf--#;\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('singleLineComment', '# asdfsd;')\n",
      "Token('singleLineComment', '# ;')\n",
      "Token('singleLineComment', '#;')\n",
      "Token('singleLineComment', '#-;')\n",
      "Token('singleLineComment', '#--;')\n",
      "Token('singleLineComment', '#-- asddsf;')\n",
      "Token('singleLineComment', '#-- asddsf #;')\n",
      "Token('singleLineComment', '#-- asddsf -#;')\n",
      "Token('string', '\"\"')\n",
      "Token('string', '\" \"')\n",
      "Token('string', '\"sdfs\"')\n",
      "Token('string', '\"sdfs \"')\n",
      "Token('string', '\"sdfs fssf\"')\n",
      "Token('string', '\"2342\"')\n",
      "Token('string', \"''\")\n",
      "Token('string', \"' '\")\n",
      "Token('string', \"'sdfs'\")\n",
      "Token('string', \"'sdfs '\")\n",
      "Token('string', \"'sdfs fssf'\")\n",
      "Token('string', \"'2342'\")\n",
      "Token('singleLineComment', '#----#;')\n",
      "Token('singleLineComment', '#-- --#;')\n",
      "Token('singleLineComment', '#--a--#;')\n",
      "Token('singleLineComment', '#--afasddsf--#;')\n",
      "Token('singleLineComment', '#--afas13 dfsa ddsf--#;')\n"
     ]
    }
   ],
   "source": [
    "for key, value in TEST_CASES.items():\n",
    "    try:\n",
    "        for token in my_lexer.lex(value):\n",
    "            print(token)\n",
    "    except:\n",
    "        print(f\"there is a lexing error at {key}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
