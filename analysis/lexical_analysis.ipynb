{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an experiment of how to use `rply.LexerGenerator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the necessary libraries\n",
    "\n",
    "- rply.LexerGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rply import LexerGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LexerGenerator()\n",
    "\n",
    "# Syntax Rules\n",
    "lg.add(\"TEMPLATE\", r\"template\")\n",
    "lg.add(\"FUNCTION\", r\"fn\")\n",
    "lg.add(\"VOID\", r\"void\")\n",
    "lg.add(\"ASSIGNMENT\", r\"=\")\n",
    "lg.add(\"SEMI_COLON\", r\";\")\n",
    "lg.add(\"FROM\", r\"from\")\n",
    "lg.add(\"TO\", r\"to\")\n",
    "lg.add(\"AS\", r\"as\")\n",
    "lg.add(\"OPEN_CURLY_BRACKET\", r\"{\")\n",
    "lg.add(\"CLOSE_CURLY_BRACKET\", r\"}\")\n",
    "lg.add(\"CHECK\", r\"check\")\n",
    "lg.add(\"RETURN_STATEMENT\", r\"finish;\")\n",
    "lg.add(\"TYPE\", r\"int|bol|arr<int>\")\n",
    "lg.add('OPEN_BRACKET', r'\\[')\n",
    "lg.add('CLOSE_BRACKET', r']')\n",
    "lg.add('COMMA', r',')\n",
    "lg.add('OPERATOR', r'>|<|={2}|!=|\\+|-{1}|\\*|/')\n",
    "lg.add('SPECIAL_CHARACTER', r'\\W')\n",
    "\n",
    "\n",
    "lg.add('BOOLEAN', r'true|false')\n",
    "lg.add('LETTER', r'[a-zA-Z]')\n",
    "lg.add('NUMBER', r'\\d+')\n",
    "\n",
    "lg.ignore(r\"\\s+\")\n",
    "\n",
    "l = lg.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('SEMI_COLON', ';')\n",
      "Token('OPERATOR', '+')\n",
      "Token('OPERATOR', '-')\n",
      "Token('OPERATOR', '-')\n",
      "Token('OPERATOR', '-')\n",
      "Token('OPERATOR', '-')\n",
      "Token('OPERATOR', '-')\n",
      "Token('OPERATOR', '-')\n",
      "Token('COMMA', ',')\n",
      "Token('OPERATOR', '!=')\n",
      "Token('BOOLEAN', 'false')\n",
      "Token('BOOLEAN', 'true')\n",
      "Token('ASSIGNMENT', '=')\n",
      "Token('ASSIGNMENT', '=')\n",
      "Token('LETTER', 'f')\n",
      "Token('LETTER', 'q')\n",
      "Token('LETTER', 'e')\n",
      "Token('AS', 'as')\n",
      "Token('LETTER', 'd')\n",
      "Token('LETTER', 'f')\n",
      "Token('NUMBER', '12')\n",
      "Token('NUMBER', '1212')\n",
      "Token('NUMBER', '1')\n",
      "Token('CLOSE_BRACKET', ']')\n",
      "Token('OPEN_BRACKET', '[')\n",
      "Token('CLOSE_BRACKET', ']')\n",
      "Token('TYPE', 'int')\n",
      "Token('LETTER', 'a')\n",
      "Token('LETTER', 'r')\n",
      "Token('LETTER', 'r')\n",
      "Token('TYPE', 'arr<int>')\n",
      "Token('TYPE', 'bol')\n",
      "Token('OPEN_CURLY_BRACKET', '{')\n",
      "Token('CLOSE_CURLY_BRACKET', '}')\n",
      "Token('AS', 'as')\n",
      "Token('CHECK', 'check')\n",
      "Token('FROM', 'from')\n"
     ]
    }
   ],
   "source": [
    "INPUT = \"; + - -- -- - , != false true == fqe asdf 12 1212 1][] int arr arr<int> bol {} as check from\" \n",
    "for token in l.lex(INPUT):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if there is no rule added, it will be an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a sample tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is how to construct a tokenizer for the following snippets\n",
    "\n",
    "```\n",
    "str txt = \"Sample String\";\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = 'str txt = \"Sample String\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lg = LexerGenerator()\n",
    "\n",
    "my_lg.add(\"dataType\", r\"str\")\n",
    "my_lg.add(\"string\", r'\".*\"')\n",
    "my_lg.add(\"variable\", r\"[a-zA-Z][a-zA-Z0-9_]*\")\n",
    "my_lg.add(\"assignment\", r\"=\")\n",
    "\n",
    "my_lg.ignore(r\"\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_l = my_lg.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('dataType', 'str')\n",
      "Token('variable', 'txt')\n",
      "Token('assignment', '=')\n",
      "Token('string', '\"Sample String\"')\n"
     ]
    }
   ],
   "source": [
    "for token in my_l.lex(INPUT):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Rules based on BNF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ebnf_1.png\" />\n",
    "<img src=\"img/ebnf_2.png\" />\n",
    "<img src=\"img/ebnf_3.png\" />\n",
    "<img src=\"img/ebnf_4.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "RULES = {\n",
    "    \"singleLineComment\": r\"^#.*;$\",\n",
    "    \"multiLineComment\": r\"^#-{2,}.*-{2,}#;$\",\n",
    "    \"string\": r\"\\\".*\\\"|'.*'\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLexerGenerator:\n",
    "    def __init__(self):\n",
    "        self.lg = LexerGenerator()\n",
    "        self.is_rule_added = False\n",
    "    def add_rules(self, rules):\n",
    "        for key, value in rules.items():\n",
    "            self.lg.add(key, value)\n",
    "        self.lg.ignore(r\"\\s+\")\n",
    "        self.is_rule_added = True\n",
    "    def build(self):\n",
    "        if self.is_rule_added:\n",
    "            return self.lg.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lexer_generator = MyLexerGenerator()\n",
    "my_lexer_generator.add_rules(RULES)\n",
    "my_lexer = my_lexer_generator.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CASES = {\n",
    "    \"singleLineComment#1\": \"# asdfsd;\",\n",
    "    \"singleLineComment#2\": \"# ;\",\n",
    "    \"singleLineComment#3\": \"#;\",\n",
    "    \"singleLineComment#4\": \"#-;\",\n",
    "    \"singleLineComment#5\": \"#--;\",\n",
    "    \"singleLineComment#6\": \"#-- asddsf;\",\n",
    "    \"singleLineComment#7\": \"#-- asddsf #;\",\n",
    "    \"singleLineComment#8\": \"#-- asddsf -#;\",\n",
    "    \"string#1\": \"\\\"\\\"\",\n",
    "    \"string#2\": \"\\\" \\\"\",\n",
    "    \"string#3\": \"\\\"sdfs\\\"\",\n",
    "    \"string#4\": \"\\\"sdfs \\\"\",\n",
    "    \"string#5\": \"\\\"sdfs fssf\\\"\",\n",
    "    \"string#6\": \"\\\"2342\\\"\",\n",
    "    \"string#7\": \"''\",\n",
    "    \"string#8\": \"' '\",\n",
    "    \"string#9\": \"'sdfs'\",\n",
    "    \"string#10\": \"'sdfs '\",\n",
    "    \"string#11\": \"'sdfs fssf'\",\n",
    "    \"string#12\": \"'2342'\",\n",
    "    \"multiLineComment#1\": \"#----#;\",\n",
    "    \"multiLineComment#2\": \"#-- --#;\",\n",
    "    \"multiLineComment#3\": \"#--a--#;\",\n",
    "    \"multiLineComment#4\": \"#--afasddsf--#;\",\n",
    "    \"multiLineComment#5\": \"#--afas13 dfsa ddsf--#;\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('singleLineComment', '# asdfsd;')\n",
      "Token('singleLineComment', '# ;')\n",
      "Token('singleLineComment', '#;')\n",
      "Token('singleLineComment', '#-;')\n",
      "Token('singleLineComment', '#--;')\n",
      "Token('singleLineComment', '#-- asddsf;')\n",
      "Token('singleLineComment', '#-- asddsf #;')\n",
      "Token('singleLineComment', '#-- asddsf -#;')\n",
      "Token('string', '\"\"')\n",
      "Token('string', '\" \"')\n",
      "Token('string', '\"sdfs\"')\n",
      "Token('string', '\"sdfs \"')\n",
      "Token('string', '\"sdfs fssf\"')\n",
      "Token('string', '\"2342\"')\n",
      "Token('string', \"''\")\n",
      "Token('string', \"' '\")\n",
      "Token('string', \"'sdfs'\")\n",
      "Token('string', \"'sdfs '\")\n",
      "Token('string', \"'sdfs fssf'\")\n",
      "Token('string', \"'2342'\")\n",
      "Token('singleLineComment', '#----#;')\n",
      "Token('singleLineComment', '#-- --#;')\n",
      "Token('singleLineComment', '#--a--#;')\n",
      "Token('singleLineComment', '#--afasddsf--#;')\n",
      "Token('singleLineComment', '#--afas13 dfsa ddsf--#;')\n"
     ]
    }
   ],
   "source": [
    "for key, value in TEST_CASES.items():\n",
    "    try:\n",
    "        for token in my_lexer.lex(value):\n",
    "            print(token)\n",
    "    except:\n",
    "        print(f\"there is a lexing error at {key}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
